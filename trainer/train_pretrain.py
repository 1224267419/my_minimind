import os
import sys

__package__ = "trainer"
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import argparse
import time
import warnings
import torch
import torch.distributed as dist
from contextlib import nullcontext
from torch import optim, nn
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data import DataLoader, DistributedSampler
from model.model import MiniMindConfig
from dataset.lm_dataset import PretrainDataset
from trainer.trainer_utils import (
    get_lr,
    Logger,
    is_main_process,
    lm_checkpoint,
    init_distributed_mode,
    setup_seed,
    init_model,
    SkipBatchSampler,
)

warnings.filterwarnings('ignore')


def train_epoch(epoch, loader, iters, start_step=0, wandb=None):
    """
    epoch : ç¬¬å‡ ä¸ªepoch
    loader :dataloader
    iters : ä¸€ä¸ªepochè¿­ä»£å¤šå°‘æ¬¡
    start_step : ç”¨äºresume
    """
    # å› ä¸ºä»£ç ä¸­ä¼ å…¥äº† loss_maskã€‚é€šå¸¸æ˜¯å› ä¸ºè¾“å…¥æ•°æ®åŒ…å« Paddingï¼ˆå¡«å……ç¬¦ï¼‰ï¼Œ
    # æˆ‘ä»¬éœ€è¦ä¿ç•™æ‰€æœ‰ Lossï¼Œç¨åæ‰‹åŠ¨ä¹˜ä»¥ Maskï¼ˆæŠŠ Padding çš„ Loss å˜æˆ 0ï¼‰ï¼Œç„¶åå†æ±‚å¹³å‡
    loss_fct = nn.CrossEntropyLoss(reduction='none')
    start_time = time.time()
    #
    for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):
        X = X.to(args.device)
        Y = Y.to(args.device)
        loss_mask = loss_mask.to(args.device)
        lr = get_lr(epoch * iters + step, iters * args.epochs, args.learning_rate)

        # å¯ä»¥ä¿®æ”¹æ¯ä¸€ä¸ªoptimçš„lr, è¿™é‡Œç›´æ¥ç»Ÿä¸€
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
            # æ··åˆç²¾åº¦:
            # çŸ©é˜µä¹˜æ³•å’Œå·ç§¯ä½¿ç”¨f16
            # softmaxå’Œæ±‚å’Œä½¿ç”¨f32
            with autocast_ctx:
                # å‰å‘ä¼ æ’­
                res = model(X)
                # lossè®¡ç®—,æ‹æ‰æ‰€æœ‰token,è®¡ç®—æ¯ä¸ªç”Ÿæˆtokençš„äº¤å‰ç†µæŸå¤±
                loss = loss_fct(res.logits.view(-1, lm_config.vocab_size), Y.view(-1))
                # 3. åº”ç”¨ Mask (è¿‡æ»¤æ‰ Padding çš„ loss)
                # loss_mask.view(-1) æŠŠ mask ä¹Ÿæ‹‰å¹³ï¼Œå½¢çŠ¶ [8]
                # loss æ˜¯åˆšæ‰ç®—å‡ºæ¥çš„æ¯ä¸ª token çš„ lossï¼Œå½¢çŠ¶ä¹Ÿæ˜¯ [8]
                loss = (loss * loss_mask.view(-1)).sum() / loss_mask.sum()
                loss = loss / args.accumulation_steps

            # 4. åå‘ä¼ æ’­
            scaler.scale(loss).backward()  # å¦‚æœç”¨äº†æ··åˆç²¾åº¦ï¼Œé€šå¸¸é…åˆ GradScaler
            if (step + 1) % args.accumulation_steps == 0:

                scaler.unscale_(optimizer)  # å…ˆè¿˜åŸæ¢¯åº¦
                # é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸,è¿›è¡Œè£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)

                scaler.step(optimizer)
                scaler.update()
                # æ¢¯åº¦è®¾ç½®ä¸ºnoneå¯ä»¥èŠ‚çœå†…å­˜
                optimizer.zero_grad(set_to_none=True)

            # logè®°å½•
            if step % args.log_interval == 0 or step == iters - 1:
                spend_time = time.time() - start_time
                current_loss = loss.item() * args.accumulation_steps  # æ¢å¤çœŸå®æŸå¤±å€¼
                current_lr = optimizer.param_groups[-1]["lr"]  # å½“å‰å­¦ä¹ ç‡

                eta_min = spend_time / (step + 1) * iters // 60 - spend_time // 60

                Logger(
                    f"Epoch:[{epoch + 1}/{args.epochs}]({step}/{iters}) loss:{current_loss:.6f} lr:{current_lr:.12f} epoch_Time:{eta_min}min:"
                )

                # è®°å½•åˆ°å®éªŒè·Ÿè¸ªç³»ç»Ÿ
                if wandb:
                    wandb.log(
                        {"loss": current_loss, "lr": current_lr, "epoch_Time": eta_min}
                    )

            if (
                step % args.save_interval == 0 or step == iters - 1
            ) and is_main_process():
                model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼

                # æ„å»ºä¿å­˜è·¯å¾„
                moe_suffix = (
                    "_moe"
                    if hasattr(lm_config, "use_moe") and lm_config.use_moe
                    else ""
                )
                ckp = f"{args.save_dir}/{args.save_weight}_{lm_config.hidden_size}{moe_suffix}.pth"

                # ğŸ“š åˆ†å¸ƒå¼æ¨¡å‹ä¿å­˜
                # DDPæ¨¡å‹éœ€è¦é€šè¿‡.moduleè®¿é—®çœŸæ­£çš„æ¨¡å‹
                if isinstance(model, torch.nn.parallel.DistributedDataParallel):
                    state_dict = model.module.state_dict()
                else:
                    state_dict = model.state_dict()

                # ğŸ“š åŠç²¾åº¦ä¿å­˜
                # å°†float32å‚æ•°è½¬ä¸ºfloat16ï¼Œå‡å°‘å­˜å‚¨ç©ºé—´
                state_dict = {k: v.half() for k, v in state_dict.items()}
                torch.save(state_dict, ckp)

                # ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€ , åœ¨utilsé‡Œé¢æœ‰
                lm_checkpoint(
                    lm_config,
                    weight=args.save_weight,
                    model=model,
                    optimizer=optimizer,
                    scaler=scaler,
                    epoch=epoch,
                    step=step,
                    wandb=wandb,
                    save_dir="checkpoints",
                )

                model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MiniMind Pretraining")
    parser.add_argument("--save_dir", type=str, default="../out", help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument(
        '--save_weight', default='pretrain', type=str, help="ä¿å­˜æƒé‡çš„å‰ç¼€å"
    )
    parser.add_argument(
        "--epochs", type=int, default=1, help="è®­ç»ƒè½®æ•°ï¼ˆå»ºè®®1è½®zeroæˆ–2-6è½®å……åˆ†è®­ç»ƒï¼‰"
    )
    parser.add_argument("--batch_size", type=int, default=32, help="batch size")
    parser.add_argument("--learning_rate", type=float, default=5e-4, help="åˆå§‹å­¦ä¹ ç‡")
    parser.add_argument(
        "--device",
        type=str,
        default="cuda:0" if torch.cuda.is_available() else "cpu",
        help="è®­ç»ƒè®¾å¤‡",
    )
    parser.add_argument("--dtype", type=str, default="bfloat16", help="æ··åˆç²¾åº¦ç±»å‹")
    parser.add_argument("--num_workers", type=int, default=8, help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")
    parser.add_argument(
        "--accumulation_steps", type=int, default=8, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°"
    )
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    parser.add_argument("--log_interval", type=int, default=100, help="æ—¥å¿—æ‰“å°é—´éš”")
    parser.add_argument("--save_interval", type=int, default=1000, help="æ¨¡å‹ä¿å­˜é—´éš”")
    parser.add_argument('--hidden_size', default=512, type=int, help="éšè—å±‚ç»´åº¦")
    parser.add_argument('--num_hidden_layers', default=8, type=int, help="éšè—å±‚æ•°é‡")
    parser.add_argument(
        '--max_seq_len',
        default=340,
        type=int,
        help="è®­ç»ƒçš„æœ€å¤§æˆªæ–­é•¿åº¦ï¼ˆä¸­æ–‡1tokenâ‰ˆ1.5~1.7å­—ç¬¦ï¼‰",
    )
    parser.add_argument(
        '--use_moe',
        default=0,
        type=int,
        choices=[0, 1],
        help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰",
    )
    parser.add_argument(
        "--data_path",
        type=str,
        default="../dataset/pretrain_hq.jsonl",
        help="é¢„è®­ç»ƒæ•°æ®è·¯å¾„",
    )
    parser.add_argument(
        '--from_weight',
        default='none',
        type=str,
        help="åŸºäºå“ªä¸ªæƒé‡è®­ç»ƒï¼Œä¸ºnoneåˆ™ä»å¤´å¼€å§‹",
    )
    parser.add_argument(
        '--from_resume',
        default=0,
        type=int,
        choices=[0, 1],
        help="æ˜¯å¦è‡ªåŠ¨æ£€æµ‹&ç»­è®­ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰",
    )
    parser.add_argument("--use_wandb", action="store_true", help="æ˜¯å¦ä½¿ç”¨wandb")
    parser.add_argument(
        "--wandb_project", type=str, default="MiniMind-Pretrain", help="wandbé¡¹ç›®å"
    )
    args = parser.parse_args()

    # ========== 1. åˆå§‹åŒ–ç¯å¢ƒå’Œéšæœºç§å­ ==========
    local_rank = init_distributed_mode()
    if dist.is_initialized():
        args.device = f"cuda:{local_rank}"
    setup_seed(42 + (dist.get_rank() if dist.is_initialized() else 0))

    # ========== 2. é…ç½®ç›®å½•ã€æ¨¡å‹å‚æ•°ã€æ£€æŸ¥ckp ==========
    os.makedirs(args.save_dir, exist_ok=True)
    lm_config = MiniMindConfig(
        hidden_size=args.hidden_size,
        num_hidden_layers=args.num_hidden_layers,
        use_moe=bool(args.use_moe),
    )
    ckp_data = (
        lm_checkpoint(lm_config, weight=args.save_weight, save_dir='../checkpoints')
        if args.from_resume == 1
        else None
    )

    # ========== 3. è®¾ç½®æ··åˆç²¾åº¦ ==========
    device_type = "cuda" if "cuda" in args.device else "cpu"
    dtype = torch.bfloat16 if args.dtype == "bfloat16" else torch.float16
    autocast_ctx = (
        nullcontext() if device_type == "cpu" else torch.cuda.amp.autocast(dtype=dtype)
    )

    # ========== 4. é…wandb ==========
    wandb = None
    if args.use_wandb and is_main_process():
        import swanlab as wandb

        wandb_id = ckp_data.get('wandb_id') if ckp_data else None
        resume = 'must' if wandb_id else None
        wandb_run_name = f"MiniMind-Pretrain-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}"
        wandb.init(
            project=args.wandb_project, name=wandb_run_name, id=wandb_id, resume=resume
        )

    # ========== 5. å®šä¹‰æ¨¡å‹ã€æ•°æ®ã€ä¼˜åŒ–å™¨ ==========
    model, tokenizer = init_model(lm_config, args.from_weight, device=args.device)
    train_ds = PretrainDataset(args.data_path, tokenizer, max_length=args.max_seq_len)
    train_sampler = DistributedSampler(train_ds) if dist.is_initialized() else None
    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == 'float16'))
    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)

    # ========== 6. ä»ckpæ¢å¤çŠ¶æ€ ==========
    start_epoch, start_step = 0, 0
    if ckp_data:
        model.load_state_dict(ckp_data['model'])
        optimizer.load_state_dict(ckp_data['optimizer'])
        scaler.load_state_dict(ckp_data['scaler'])
        start_epoch = ckp_data['epoch']
        start_step = ckp_data.get('step', 0)

    # ========== 7. DDPåŒ…æ¨¡å‹ ==========
    if dist.is_initialized():
        model._ddp_params_and_buffers_to_ignore = {"freqs_cos", "freqs_sin"}
        model = DistributedDataParallel(model, device_ids=[local_rank])

    # ========== 8. å¼€å§‹è®­ç»ƒ ==========
    for epoch in range(start_epoch, args.epochs):
        train_sampler and train_sampler.set_epoch(epoch)
        if epoch == start_epoch and start_step > 0:  # ç¬¬ä¸€ä¸ªepochä¸”å­˜åœ¨æ£€æŸ¥ç‚¹
            batch_sampler = SkipBatchSampler(
                train_sampler or range(len(train_ds)), args.batch_size, start_step + 1
            )
            loader = DataLoader(
                train_ds,
                batch_sampler=batch_sampler,
                num_workers=args.num_workers,
                pin_memory=True,
            )
            Logger(
                f'Epoch [{epoch + 1}/{args.epochs}]: è·³è¿‡å‰{start_step}ä¸ªstepï¼Œä»step {start_step + 1}å¼€å§‹'
            )
            train_epoch(epoch, loader, len(loader) + start_step + 1, start_step, wandb)
        else:  # é»˜è®¤ä»å¤´å¼€å§‹
            loader = DataLoader(
                train_ds,
                batch_size=args.batch_size,
                shuffle=(train_sampler is None),
                sampler=train_sampler,
                num_workers=args.num_workers,
                pin_memory=True,
            )
            train_epoch(epoch, loader, len(loader), 0, wandb)

    # ========== 9. æ¸…ç†åˆ†å¸ƒè¿›ç¨‹ ==========
    if dist.is_initialized():
        dist.destroy_process_group()
